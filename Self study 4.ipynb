{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self study 1\n",
    "\n",
    "In this self study you should work on the code examples below together with the associated questions. The notebook illustrates a basic neural network implementation, where we implement most of the relevant functions from scratch, except for the calculation of the gradients, for which we rely on the functionality provided by <a urel=\"https://pytorch.org\">PyTorch</a>. \n",
    "\n",
    "Since we rely on PyTorch functionality, we will also be using PyTorch's tensor data structure. This data structure operates in much the same way as numpy's ndarrays. You can find a brief introduction to PyTorch's tensors <a href=\"https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html\">here</a>.  \n",
    "\n",
    "\n",
    "\n",
    "The code illustrates the key concepts involved in the learning neural network. Go carefully through the code before starting to answer the questions at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the modules used in this selfstudy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through torch load the MNIST data set, which we will use in this self study. The MNIST database consists of grey scale images of handwritten digits. Each image is of size $28\\times 28$; see figure below for an illustration. The data set is divided into a training set consisting of $60000$ images and a test set with $10000$ images; in both\n",
    "data sets the images are labeled with the correct digits. If interested, you can find more information about the MNIST data set at http://yann.lecun.com/exdb/mnist/, including accuracy results for various machine learning methods.\n",
    "\n",
    "![MNIST DATA](MNIST-dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the data loader provided by torch we have an easy way of loading in data in batches (here of size 64). We can also make various other transformation of the data, such as normalization. The details for loading the data are not really that important, but if yiu are curious, you can find a quick introduction <a href=\"https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])),\n",
    "        batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, we are loading data in batches, which is subsequently used during optimization. We didn't discuss batch based learning during the last lecture, but please take a look at Slides 24 and 25 to get the gist of the approach. We will cover it in more detail during the next lecture.   \n",
    "\n",
    "Each batch is a list of two elements. The first element encodes the digit and has dimensions [64,1,28,28] (the figures are greyscale with no rbg channel, hence the '1'), and the second element contains the class/label information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch dimension (digit): torch.Size([64, 1, 28, 28])\n",
      "Batch dimension (target): torch.Size([64])\n",
      "Target: 1 with shape torch.Size([])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(f\"Batch dimension (digit): {batch[0].shape}\")\n",
    "print(f\"Batch dimension (target): {batch[1].shape}\")\n",
    "digit_batch = batch[0]\n",
    "img = digit_batch[0,:]\n",
    "pyplot.imshow(img.reshape((28, 28)), cmap=\"gray\")\n",
    "print(f\"Target: {batch[1][0]} with shape {batch[1][0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With PyTorch we can specify that the tensors require gradients. This will make PyTorch record all operations performed on the tensors, so that we can afterwards calculate the gradients automatically using back propagation. See also the code example from the last lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first part of this self study we will specify a neural network, which will encode a softmax function (see, e.g., Page 180 in <a href=\"https://www.deeplearningbook.org/contents/mlp.html\">The Deep Learning book</a>). For this we need a (randomly initialized) weight matrix and a bias, and for both of them we need their gradients wrt. our error function (yet to be defined) in order to perform learning. Note that to facilitate matrix multiplication we will flatten our image from $28\\times 28$ to $784$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.randn(784, 10) / np.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out model specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return x.exp() / x.exp().sum(-1).unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "    return softmax(xb @ weights + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our model (with our randomly initialized weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([64, 784])\n",
      "Prediction on first image tensor([0.0690, 0.0776, 0.0578, 0.2733, 0.0330, 0.0454, 0.0680, 0.1618, 0.1768,\n",
      "        0.0373], grad_fn=<SelectBackward0>)\n",
      "Corresponding classification: 3\n"
     ]
    }
   ],
   "source": [
    "# We flatten the digit representation so that it is consistent with the weight matrix\n",
    "xb = digit_batch.flatten(start_dim=1)\n",
    "print(f\"Batch shape: {xb.shape}\")\n",
    "preds = model(xb)\n",
    "print(f\"Prediction on first image {preds[0]}\")\n",
    "print(f\"Corresponding classification: {preds[0].argmax()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define our loss function, in this case the log-loss (or negative log-likelihood):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6341, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def nll(input, target):\n",
    "    return (-input[range(target.shape[0]), target].log()).mean() \n",
    "loss_func = nll\n",
    "\n",
    "# Make a test calculation\n",
    "yb = batch[1]\n",
    "print(loss_func(preds,yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we are interested in the accuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model on batch (with random weights): 0.03125\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy of model on batch (with random weights): {accuracy(preds, yb)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to combine it all and perform learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, B-idx: 0, Training loss: 2.5153433915648633\n",
      "Epoch: 0, B-idx: 50, Training loss: 0.8358087456429691\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m train_losses = []\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/MlSelf/.venv/lib64/python3.13/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/MlSelf/.venv/lib64/python3.13/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/MlSelf/.venv/lib64/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/MlSelf/.venv/lib64/python3.13/site-packages/torchvision/datasets/mnist.py:146\u001b[39m, in \u001b[36mMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    143\u001b[39m img = Image.fromarray(img.numpy(), mode=\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/MlSelf/.venv/lib64/python3.13/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/MlSelf/.venv/lib64/python3.13/site-packages/torchvision/transforms/transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/MlSelf/.venv/lib64/python3.13/site-packages/torchvision/transforms/functional.py:176\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    174\u001b[39m img = img.permute((\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)).contiguous()\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.ByteTensor):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m.div(\u001b[32m255\u001b[39m)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "epochs = 1  # how many epochs to train for\n",
    "lr = 0.01  # learning rate\n",
    "\n",
    "train_losses = []\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for batch_idx, (xb, yb) in enumerate(train_loader):\n",
    "\n",
    "        xb = xb.squeeze().flatten(start_dim=1)\n",
    "        pred = model(xb)\n",
    "        # We specify the loss\n",
    "        loss = loss_func(pred, yb)\n",
    "        # and perform backpropagation    \n",
    "        loss.backward()\n",
    "        # Lastly we update the weights and bias (torch.no_grad() ensures that no gradient \n",
    "        # calculations are taking place in this part of the code)\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "\n",
    "            # After updating we set the gradients to zero so that we ar eready for the next round\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "\n",
    "            if batch_idx % 50 == 0:\n",
    "                with torch.no_grad():\n",
    "                    train_loss = np.mean([loss_func(model(txb.squeeze().flatten(start_dim=1)), tyb).item() for txb, tyb in train_loader])\n",
    "                    print(f\"Epoch: {epoch}, B-idx: {batch_idx}, Training loss: {train_loss}\")\n",
    "                    train_losses.append(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the evolution of the training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4ab7b10b90>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(train_losses)), train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ \n",
    "1. Experiment with different variations of the gradient descent implementation; try varying the learning rate and the batch size. Assuming that you have a fixed time budget (say 2 minutes for learning), what can we then say about the effect of changing the parameters?\n",
    "2. Implement momentum in the learning algorithm. How does it affect the results?\n",
    "3. Try with different initialization schemes for the parameters (e.g. allowing for larger values). How does it affect the behavior of the algorithm?\n",
    "4. Analyze the behavior of the algorithm on the test set and implement a method for evaluating the accuracy over the entire training/test set.\n",
    "5. If you feel adventorous, you are welcome to try to specify and experiment with other more advanced model architectures (cell 23). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])),\n",
    "        batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, B-idx: 0, Training loss: inf\n",
      "Epoch: 0, B-idx: 50, Training loss: nan\n"
     ]
    }
   ],
   "source": [
    "# Reset model\n",
    "weights = torch.randn(784, 10) / np.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "epochs = 1  # how many epochs to train for\n",
    "lr = 1  # learning rate\n",
    "\n",
    "train_losses = []\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for batch_idx, (xb, yb) in enumerate(train_loader):\n",
    "\n",
    "        xb = xb.squeeze().flatten(start_dim=1)\n",
    "        pred = model(xb)\n",
    "        # We specify the loss\n",
    "        loss = loss_func(pred, yb)\n",
    "        # and perform backpropagation    \n",
    "        loss.backward()\n",
    "        # Lastly we update the weights and bias (torch.no_grad() ensures that no gradient \n",
    "        # calculations are taking place in this part of the code)\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "\n",
    "            # After updating we set the gradients to zero so that we ar eready for the next round\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "\n",
    "            if batch_idx % 50 == 0:\n",
    "                with torch.no_grad():\n",
    "                    train_loss = np.mean([loss_func(model(txb.squeeze().flatten(start_dim=1)), tyb).item() for txb, tyb in train_loader])\n",
    "                    print(f\"Epoch: {epoch}, B-idx: {batch_idx}, Training loss: {train_loss}\")\n",
    "                    train_losses.append(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction on first image tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Corresponding classification: 0\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64])\n",
      "tensor(nan, grad_fn=<MeanBackward0>)\n",
      "Accuracy of model on batch (with random weights): 0.0625\n"
     ]
    }
   ],
   "source": [
    "xb = digit_batch.flatten(start_dim=1)\n",
    "preds = model(xb)\n",
    "print(f\"Prediction on first image {preds[0]}\")\n",
    "print(f\"Corresponding classification: {preds[0].argmax()}\")\n",
    "\n",
    "# Make a test calculation\n",
    "yb = batch[1]\n",
    "print(preds.size())\n",
    "print(yb.size())\n",
    "print(loss_func(preds,yb))\n",
    "\n",
    "print(f\"Accuracy of model on batch (with random weights): {accuracy(preds, yb)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "Learning rate:  0.1,    0.1,    0.1,    0.1,    0.01,   0.01,   0.01,   0.01\n",
    "Batch size:     64,     512,    1024,   1024,   1024,   4096,   4096,   1024\n",
    "Epochs:         1,      1,      1,      10,     1,      1,      18,     10\n",
    "Accuracy:       0.875,  0.953,  0.953,  0.984,  0.921   0.671,  0.9375, 0.953\n",
    "Time:           1:12,   0:13,   0:10,   1:46,   0:10    0:07,   2:07,   1:46\n",
    "\n",
    "Learning rate: 1 NaN loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.9, base_loss=nn.MSELoss()):\n",
    "        super(MomentumLoss, self).__init__()\n",
    "        self.alpha = alpha  # Momentum factor\n",
    "        self.base_loss = nll  # The underlying loss function (e.g., MSE, CrossEntropy)\n",
    "        self.momentum = None  # Initialize momentum to None\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        Computes the momentum-based loss.\n",
    "\n",
    "        Args:\n",
    "            outputs (torch.Tensor): Model outputs.\n",
    "            targets (torch.Tensor): Target values.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The momentum-adjusted loss.\n",
    "        \"\"\"\n",
    "        current_loss = self.base_loss(outputs, targets)\n",
    "\n",
    "        if self.momentum is None:\n",
    "            self.momentum = current_loss.detach()  # Initialize momentum with the first loss\n",
    "        else:\n",
    "            self.momentum = self.alpha * self.momentum + (1 - self.alpha) * current_loss.detach()\n",
    "\n",
    "        return current_loss + (self.momentum - current_loss.detach()) # or just return current_loss + (self.momentum - current_loss.detach()).detach() if you want to completely stop gradients from flowing through the momentum.\n",
    "\n",
    "# Instantiate the momentum loss\n",
    "momentum_loss = MomentumLoss(alpha=0.8, base_loss=nn.MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])),\n",
    "        batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, B-idx: 0, Training loss: 1.9965021044521007\n",
      "Epoch: 0, B-idx: 50, Training loss: 0.39528934430267854\n",
      "Epoch: 1, B-idx: 0, Training loss: 0.3793098653777171\n",
      "Epoch: 1, B-idx: 50, Training loss: 0.3447726929591874\n",
      "Epoch: 2, B-idx: 0, Training loss: 0.34009054606243716\n",
      "Epoch: 2, B-idx: 50, Training loss: 0.3244131798461332\n",
      "Epoch: 3, B-idx: 0, Training loss: 0.32037492121680305\n",
      "Epoch: 3, B-idx: 50, Training loss: 0.31057351524546994\n",
      "Epoch: 4, B-idx: 0, Training loss: 0.309336090997114\n",
      "Epoch: 4, B-idx: 50, Training loss: 0.30166865708464286\n",
      "Epoch: 5, B-idx: 0, Training loss: 0.30294078487460896\n",
      "Epoch: 5, B-idx: 50, Training loss: 0.2993478891202959\n",
      "Epoch: 6, B-idx: 0, Training loss: 0.29560111740888173\n",
      "Epoch: 6, B-idx: 50, Training loss: 0.29158452456280337\n",
      "Epoch: 7, B-idx: 0, Training loss: 0.2919890466382948\n",
      "Epoch: 7, B-idx: 50, Training loss: 0.28871349763062043\n",
      "Epoch: 8, B-idx: 0, Training loss: 0.2884317575874975\n",
      "Epoch: 8, B-idx: 50, Training loss: 0.2849437266083087\n",
      "Epoch: 9, B-idx: 0, Training loss: 0.2840566488645844\n",
      "Epoch: 9, B-idx: 50, Training loss: 0.2830981642512952\n"
     ]
    }
   ],
   "source": [
    "vel = torch.zeros(1, requires_grad=False)\n",
    "# Reset model\n",
    "weights = torch.randn(784, 10) / np.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "epochs = 10  # how many epochs to train for\n",
    "lr = 0.1  # learning rate\n",
    "\n",
    "train_losses = []\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for batch_idx, (xb, yb) in enumerate(train_loader):\n",
    "\n",
    "        xb = xb.squeeze().flatten(start_dim=1)\n",
    "        pred = model(xb)\n",
    "        # We specify the loss\n",
    "        loss = momentum_loss(pred, yb)\n",
    "        vel = loss\n",
    "        # and perform backpropagation    \n",
    "        loss.backward()\n",
    "        # Lastly we update the weights and bias (torch.no_grad() ensures that no gradient \n",
    "        # calculations are taking place in this part of the code)\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "\n",
    "            # After updating we set the gradients to zero so that we ar eready for the next round\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "\n",
    "            if batch_idx % 50 == 0:\n",
    "                with torch.no_grad():\n",
    "                    train_loss = np.mean([momentum_loss(model(txb.squeeze().flatten(start_dim=1)), tyb).item() for txb, tyb in train_loader])\n",
    "                    print(f\"Epoch: {epoch}, B-idx: {batch_idx}, Training loss: {train_loss}\")\n",
    "                    train_losses.append(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction on first image tensor([6.5158e-09, 9.9396e-01, 2.9139e-04, 4.0160e-03, 9.0853e-06, 1.9325e-04,\n",
      "        5.9088e-05, 3.4189e-04, 6.7643e-04, 4.5525e-04],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Corresponding classification: 1\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64])\n",
      "tensor(0.3715, grad_fn=<MeanBackward0>)\n",
      "Accuracy of model on batch (with random weights): 0.90625\n"
     ]
    }
   ],
   "source": [
    "xb = digit_batch.flatten(start_dim=1)\n",
    "preds = model(xb)\n",
    "print(f\"Prediction on first image {preds[0]}\")\n",
    "print(f\"Corresponding classification: {preds[0].argmax()}\")\n",
    "\n",
    "# Make a test calculation\n",
    "yb = batch[1]\n",
    "print(preds.size())\n",
    "print(yb.size())\n",
    "print(loss_func(preds,yb))\n",
    "\n",
    "print(f\"Accuracy of model on batch (with random weights): {accuracy(preds, yb)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addendum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the exercise and code above, we only took modest advantage of the functionality PyTorch has to offer when it comes to specifying and training neural networks. Below I have therefore tried to illustrated another approach for modeling the MNIST data, but now using a concolutional neural network. You can find a brief but well-written intorduction to convolutional neural networks here:\n",
    "\n",
    "* http://colah.github.io/posts/2014-07-Conv-Nets-Modular/\n",
    "\n",
    "In particular, we will work with the _torch.nn_ module provided by PyTorch. A short introduction to this module and how to define neural networks in PyTorch can be found at\n",
    "\n",
    "* https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\n",
    "\n",
    "* https://pytorch.org/tutorials/beginner/nn_tutorial.html\n",
    "\n",
    "You may either go through these tutorials before move on or consult them when needed as you move forward in the notebook. The former tutorial is part of a general tutorial package to PyTorch, which can be found at (this also includes a nice introduction to tensors in PyTorch)\n",
    "\n",
    "* https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be a bit more careful with our data. Specifically, we will divide the data into a training, validation, and test, and use the training and validation set for model learning (in the previous self study we did not have a validation set). \n",
    "\n",
    "The data set is created by setting aside a randomly chosen subset of the data, where the splitting point is found using the help function *split_indicies* below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 48000\n",
      "Number of validation examples: 12000\n"
     ]
    }
   ],
   "source": [
    "def split_indicies(n, val_pct):\n",
    "    # Size of validation set\n",
    "    n_val = int(n*val_pct)\n",
    "    # Random permutation\n",
    "    idxs = np.random.permutation(n)\n",
    "    # Return first indexes for the validation set\n",
    "    return idxs[n_val:], idxs[:n_val]\n",
    "\n",
    "# Load the data\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "\n",
    "# Get the indicies for the training data and test data (the validation set will consists of 20% of the data)\n",
    "train_idxs, val_idxs = split_indicies(len(train_dataset), 0.2)\n",
    "\n",
    "# Define samplers (used by Dataloader) to the two sets of indicies\n",
    "train_sampler = SubsetRandomSampler(train_idxs)\n",
    "val_sampler = SubsetRandomSampler(val_idxs)\n",
    "\n",
    "# Specify data loaders for our training and test set (same functionality as in the previous self study)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, sampler=val_sampler)\n",
    "\n",
    "print(f\"Number of training examples: {len(train_idxs)}\")\n",
    "print(f\"Number of validation examples: {len(val_idxs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set is loaden is the usual fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])),\n",
    "        batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the model\n",
    "\n",
    "When using the _torch.nn_ for specifying our model we subclass the _nn.Module_. The model thus holds all the parameters of the model (see the _init_ function) as well as a specification of the forward step. We don't have to keep track of the backward pass, as PyTorch handles this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define a convolution operator with 1 input channel, 15 output channels and a kernel size of 5x5\n",
    "        self.conv1 = nn.Conv2d(1, 15, 5)\n",
    "        # Since we are not doing padding (see Lecture 2, Slide 38) the width of the following layer is reduced; for\n",
    "        # each channel the resulting dimension is 24x24. We feed the resulting representation through a linear \n",
    "        # layer, giving 10 values as output - one for each digit.\n",
    "        self.fc = nn.Linear(15 * 24 * 24, 10)\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, xb):\n",
    "\n",
    "        # Reshape the input tensor; '-1' indicates that PyTorch will fill-in this \n",
    "        # dimension, whereas the '1' indicates that we only have one color channel. \n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        # Apply convolution and pass the result through a ReLU function\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        # Reshape the representation\n",
    "        xb = xb.view(-1, 15*24*24)\n",
    "        # Apply the linear layer\n",
    "        xb = self.fc(xb)\n",
    "        # and set the result as the output. Note that we don't take a softmax as this is handled internally in the \n",
    "        # loss function defined below.\n",
    "        self.out = xb\n",
    "\n",
    "        return xb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning and evaluating the model\n",
    "\n",
    "For learning the model, we will use the following function which performs one iteration over the training data. The function also takes an _epoch_ argument, but this is only used for reporting on the learning progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_fn, epoch):\n",
    "    # Tell PyTorch that this function is part of the training\n",
    "    model.train()\n",
    "\n",
    "    # As optimizer we use stochastic gradient descent as defined by PyTorch. PyTorch also includes a variety \n",
    "    # of other optimizers \n",
    "    learning_rate = 0.01\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Iterate over the training set, one batch at the time, as in the previous self sudy\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Get the prediction\n",
    "        y_pred = model(data)\n",
    "        \n",
    "        # Remember to zero the gradients so that they don't accumulate\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Calculate the loss and and the gradients  \n",
    "        loss = loss_fn(y_pred, target)\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize the parameters by taking one 'step' with the optimizer\n",
    "        opt.step()\n",
    "\n",
    "        # For every 10th batch we output a bit of info\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.sampler),\n",
    "                       100. * batch_idx * len(data) / len(train_loader.sampler), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we also want to validate our model. To do this we define the function below, which takes a data_loader (either the validation or test set) and reports the model's accuracy and loss on that data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(_model, data_loader, loss_fn):\n",
    "    # Tell PyTorch that we are performing evaluation\n",
    "    _model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            output = _model(data)\n",
    "            test_loss += loss_fn(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "\n",
    "    print('\\nTest/validation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(data_loader.sampler),\n",
    "        100. * correct / len(data_loader.sampler)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A couple of helper functions\n",
    "\n",
    "Learning a deep neural network can be time consuming, and it might therefore be nice to be able to save and load previously learned models (see also https://pytorch.org/tutorials/beginner/saving_loading_models.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(file_name, model):\n",
    "    torch.save(model, file_name)\n",
    "\n",
    "def load_model(file_name):\n",
    "    model = torch.load(file_name)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping things up\n",
    "\n",
    "Finally, we will do the actual learning of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of passes that will be made over the training set\n",
    "num_epochs = 2\n",
    "# torch.nn defines several useful loss-functions, which we will take advantage of here (see Lecture 1, Slide 11, Log-loss).\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:\n",
      "MNIST_CNN(\n",
      "  (conv1): Conv2d(1, 15, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc): Linear(in_features=8640, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model class\n",
    "model = MNIST_CNN()\n",
    "# and get some information about the structure\n",
    "print('Model structure:')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate over the data set\n",
    "\n",
    "We iterate over the data set for *num_epochs* number of iterations. At each iteration we also calculate the loss/accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/48000 (0%)]\tLoss: 2.252799\n",
      "Train Epoch: 0 [640/48000 (1%)]\tLoss: 1.263240\n",
      "Train Epoch: 0 [1280/48000 (3%)]\tLoss: 1.086223\n",
      "Train Epoch: 0 [1920/48000 (4%)]\tLoss: 0.659278\n",
      "Train Epoch: 0 [2560/48000 (5%)]\tLoss: 0.541931\n",
      "Train Epoch: 0 [3200/48000 (7%)]\tLoss: 0.548204\n",
      "Train Epoch: 0 [3840/48000 (8%)]\tLoss: 0.460584\n",
      "Train Epoch: 0 [4480/48000 (9%)]\tLoss: 0.572006\n",
      "Train Epoch: 0 [5120/48000 (11%)]\tLoss: 0.423151\n",
      "Train Epoch: 0 [5760/48000 (12%)]\tLoss: 0.353434\n",
      "Train Epoch: 0 [6400/48000 (13%)]\tLoss: 0.311678\n",
      "Train Epoch: 0 [7040/48000 (15%)]\tLoss: 0.313688\n",
      "Train Epoch: 0 [7680/48000 (16%)]\tLoss: 0.391460\n",
      "Train Epoch: 0 [8320/48000 (17%)]\tLoss: 0.258515\n",
      "Train Epoch: 0 [8960/48000 (19%)]\tLoss: 0.490335\n",
      "Train Epoch: 0 [9600/48000 (20%)]\tLoss: 0.372392\n",
      "Train Epoch: 0 [10240/48000 (21%)]\tLoss: 0.280571\n",
      "Train Epoch: 0 [10880/48000 (23%)]\tLoss: 0.257049\n",
      "Train Epoch: 0 [11520/48000 (24%)]\tLoss: 0.282138\n",
      "Train Epoch: 0 [12160/48000 (25%)]\tLoss: 0.284211\n",
      "Train Epoch: 0 [12800/48000 (27%)]\tLoss: 0.180528\n",
      "Train Epoch: 0 [13440/48000 (28%)]\tLoss: 0.442509\n",
      "Train Epoch: 0 [14080/48000 (29%)]\tLoss: 0.330890\n",
      "Train Epoch: 0 [14720/48000 (31%)]\tLoss: 0.187640\n",
      "Train Epoch: 0 [15360/48000 (32%)]\tLoss: 0.216812\n",
      "Train Epoch: 0 [16000/48000 (33%)]\tLoss: 0.283717\n",
      "Train Epoch: 0 [16640/48000 (35%)]\tLoss: 0.245245\n",
      "Train Epoch: 0 [17280/48000 (36%)]\tLoss: 0.179639\n",
      "Train Epoch: 0 [17920/48000 (37%)]\tLoss: 0.319789\n",
      "Train Epoch: 0 [18560/48000 (39%)]\tLoss: 0.319702\n",
      "Train Epoch: 0 [19200/48000 (40%)]\tLoss: 0.257249\n",
      "Train Epoch: 0 [19840/48000 (41%)]\tLoss: 0.213628\n",
      "Train Epoch: 0 [20480/48000 (43%)]\tLoss: 0.300923\n",
      "Train Epoch: 0 [21120/48000 (44%)]\tLoss: 0.432989\n",
      "Train Epoch: 0 [21760/48000 (45%)]\tLoss: 0.312938\n",
      "Train Epoch: 0 [22400/48000 (47%)]\tLoss: 0.341195\n",
      "Train Epoch: 0 [23040/48000 (48%)]\tLoss: 0.403260\n",
      "Train Epoch: 0 [23680/48000 (49%)]\tLoss: 0.140465\n",
      "Train Epoch: 0 [24320/48000 (51%)]\tLoss: 0.197052\n",
      "Train Epoch: 0 [24960/48000 (52%)]\tLoss: 0.316744\n",
      "Train Epoch: 0 [25600/48000 (53%)]\tLoss: 0.153719\n",
      "Train Epoch: 0 [26240/48000 (55%)]\tLoss: 0.360401\n",
      "Train Epoch: 0 [26880/48000 (56%)]\tLoss: 0.378522\n",
      "Train Epoch: 0 [27520/48000 (57%)]\tLoss: 0.296692\n",
      "Train Epoch: 0 [28160/48000 (59%)]\tLoss: 0.257025\n",
      "Train Epoch: 0 [28800/48000 (60%)]\tLoss: 0.242701\n",
      "Train Epoch: 0 [29440/48000 (61%)]\tLoss: 0.247066\n",
      "Train Epoch: 0 [30080/48000 (63%)]\tLoss: 0.208662\n",
      "Train Epoch: 0 [30720/48000 (64%)]\tLoss: 0.164627\n",
      "Train Epoch: 0 [31360/48000 (65%)]\tLoss: 0.137360\n",
      "Train Epoch: 0 [32000/48000 (67%)]\tLoss: 0.221532\n",
      "Train Epoch: 0 [32640/48000 (68%)]\tLoss: 0.122718\n",
      "Train Epoch: 0 [33280/48000 (69%)]\tLoss: 0.168505\n",
      "Train Epoch: 0 [33920/48000 (71%)]\tLoss: 0.172512\n",
      "Train Epoch: 0 [34560/48000 (72%)]\tLoss: 0.177517\n",
      "Train Epoch: 0 [35200/48000 (73%)]\tLoss: 0.374833\n",
      "Train Epoch: 0 [35840/48000 (75%)]\tLoss: 0.244854\n",
      "Train Epoch: 0 [36480/48000 (76%)]\tLoss: 0.262382\n",
      "Train Epoch: 0 [37120/48000 (77%)]\tLoss: 0.094044\n",
      "Train Epoch: 0 [37760/48000 (79%)]\tLoss: 0.444150\n",
      "Train Epoch: 0 [38400/48000 (80%)]\tLoss: 0.182655\n",
      "Train Epoch: 0 [39040/48000 (81%)]\tLoss: 0.492859\n",
      "Train Epoch: 0 [39680/48000 (83%)]\tLoss: 0.224654\n",
      "Train Epoch: 0 [40320/48000 (84%)]\tLoss: 0.187253\n",
      "Train Epoch: 0 [40960/48000 (85%)]\tLoss: 0.169076\n",
      "Train Epoch: 0 [41600/48000 (87%)]\tLoss: 0.128631\n",
      "Train Epoch: 0 [42240/48000 (88%)]\tLoss: 0.152113\n",
      "Train Epoch: 0 [42880/48000 (89%)]\tLoss: 0.292802\n",
      "Train Epoch: 0 [43520/48000 (91%)]\tLoss: 0.335730\n",
      "Train Epoch: 0 [44160/48000 (92%)]\tLoss: 0.237513\n",
      "Train Epoch: 0 [44800/48000 (93%)]\tLoss: 0.253340\n",
      "Train Epoch: 0 [45440/48000 (95%)]\tLoss: 0.205673\n",
      "Train Epoch: 0 [46080/48000 (96%)]\tLoss: 0.287576\n",
      "Train Epoch: 0 [46720/48000 (97%)]\tLoss: 0.438994\n",
      "Train Epoch: 0 [47360/48000 (99%)]\tLoss: 0.138560\n",
      "\n",
      "Test/validation set: Average loss: 0.0006, Accuracy: 11287/12000 (94%)\n",
      "\n",
      "Train Epoch: 1 [0/48000 (0%)]\tLoss: 0.136687\n",
      "Train Epoch: 1 [640/48000 (1%)]\tLoss: 0.148638\n",
      "Train Epoch: 1 [1280/48000 (3%)]\tLoss: 0.344422\n",
      "Train Epoch: 1 [1920/48000 (4%)]\tLoss: 0.174984\n",
      "Train Epoch: 1 [2560/48000 (5%)]\tLoss: 0.165870\n",
      "Train Epoch: 1 [3200/48000 (7%)]\tLoss: 0.124865\n",
      "Train Epoch: 1 [3840/48000 (8%)]\tLoss: 0.350896\n",
      "Train Epoch: 1 [4480/48000 (9%)]\tLoss: 0.095787\n",
      "Train Epoch: 1 [5120/48000 (11%)]\tLoss: 0.093572\n",
      "Train Epoch: 1 [5760/48000 (12%)]\tLoss: 0.078425\n",
      "Train Epoch: 1 [6400/48000 (13%)]\tLoss: 0.088912\n",
      "Train Epoch: 1 [7040/48000 (15%)]\tLoss: 0.115682\n",
      "Train Epoch: 1 [7680/48000 (16%)]\tLoss: 0.098531\n",
      "Train Epoch: 1 [8320/48000 (17%)]\tLoss: 0.286149\n",
      "Train Epoch: 1 [8960/48000 (19%)]\tLoss: 0.093941\n",
      "Train Epoch: 1 [9600/48000 (20%)]\tLoss: 0.260383\n",
      "Train Epoch: 1 [10240/48000 (21%)]\tLoss: 0.210784\n",
      "Train Epoch: 1 [10880/48000 (23%)]\tLoss: 0.271635\n",
      "Train Epoch: 1 [11520/48000 (24%)]\tLoss: 0.134776\n",
      "Train Epoch: 1 [12160/48000 (25%)]\tLoss: 0.116998\n",
      "Train Epoch: 1 [12800/48000 (27%)]\tLoss: 0.079019\n",
      "Train Epoch: 1 [13440/48000 (28%)]\tLoss: 0.126596\n",
      "Train Epoch: 1 [14080/48000 (29%)]\tLoss: 0.116784\n",
      "Train Epoch: 1 [14720/48000 (31%)]\tLoss: 0.159679\n",
      "Train Epoch: 1 [15360/48000 (32%)]\tLoss: 0.066805\n",
      "Train Epoch: 1 [16000/48000 (33%)]\tLoss: 0.225973\n",
      "Train Epoch: 1 [16640/48000 (35%)]\tLoss: 0.092248\n",
      "Train Epoch: 1 [17280/48000 (36%)]\tLoss: 0.139978\n",
      "Train Epoch: 1 [17920/48000 (37%)]\tLoss: 0.185325\n",
      "Train Epoch: 1 [18560/48000 (39%)]\tLoss: 0.132748\n",
      "Train Epoch: 1 [19200/48000 (40%)]\tLoss: 0.371061\n",
      "Train Epoch: 1 [19840/48000 (41%)]\tLoss: 0.262983\n",
      "Train Epoch: 1 [20480/48000 (43%)]\tLoss: 0.241270\n",
      "Train Epoch: 1 [21120/48000 (44%)]\tLoss: 0.105288\n",
      "Train Epoch: 1 [21760/48000 (45%)]\tLoss: 0.036118\n",
      "Train Epoch: 1 [22400/48000 (47%)]\tLoss: 0.086564\n",
      "Train Epoch: 1 [23040/48000 (48%)]\tLoss: 0.109286\n",
      "Train Epoch: 1 [23680/48000 (49%)]\tLoss: 0.101627\n",
      "Train Epoch: 1 [24320/48000 (51%)]\tLoss: 0.144682\n",
      "Train Epoch: 1 [24960/48000 (52%)]\tLoss: 0.194857\n",
      "Train Epoch: 1 [25600/48000 (53%)]\tLoss: 0.033359\n",
      "Train Epoch: 1 [26240/48000 (55%)]\tLoss: 0.319706\n",
      "Train Epoch: 1 [26880/48000 (56%)]\tLoss: 0.083905\n",
      "Train Epoch: 1 [27520/48000 (57%)]\tLoss: 0.066681\n",
      "Train Epoch: 1 [28160/48000 (59%)]\tLoss: 0.219735\n",
      "Train Epoch: 1 [28800/48000 (60%)]\tLoss: 0.190409\n",
      "Train Epoch: 1 [29440/48000 (61%)]\tLoss: 0.173086\n",
      "Train Epoch: 1 [30080/48000 (63%)]\tLoss: 0.246823\n",
      "Train Epoch: 1 [30720/48000 (64%)]\tLoss: 0.118034\n",
      "Train Epoch: 1 [31360/48000 (65%)]\tLoss: 0.290848\n",
      "Train Epoch: 1 [32000/48000 (67%)]\tLoss: 0.374604\n",
      "Train Epoch: 1 [32640/48000 (68%)]\tLoss: 0.172676\n",
      "Train Epoch: 1 [33280/48000 (69%)]\tLoss: 0.038442\n",
      "Train Epoch: 1 [33920/48000 (71%)]\tLoss: 0.227589\n",
      "Train Epoch: 1 [34560/48000 (72%)]\tLoss: 0.110604\n",
      "Train Epoch: 1 [35200/48000 (73%)]\tLoss: 0.178852\n",
      "Train Epoch: 1 [35840/48000 (75%)]\tLoss: 0.144118\n",
      "Train Epoch: 1 [36480/48000 (76%)]\tLoss: 0.080062\n",
      "Train Epoch: 1 [37120/48000 (77%)]\tLoss: 0.248485\n",
      "Train Epoch: 1 [37760/48000 (79%)]\tLoss: 0.083826\n",
      "Train Epoch: 1 [38400/48000 (80%)]\tLoss: 0.126555\n",
      "Train Epoch: 1 [39040/48000 (81%)]\tLoss: 0.072008\n",
      "Train Epoch: 1 [39680/48000 (83%)]\tLoss: 0.125667\n",
      "Train Epoch: 1 [40320/48000 (84%)]\tLoss: 0.062331\n",
      "Train Epoch: 1 [40960/48000 (85%)]\tLoss: 0.212333\n",
      "Train Epoch: 1 [41600/48000 (87%)]\tLoss: 0.255952\n",
      "Train Epoch: 1 [42240/48000 (88%)]\tLoss: 0.094569\n",
      "Train Epoch: 1 [42880/48000 (89%)]\tLoss: 0.229438\n",
      "Train Epoch: 1 [43520/48000 (91%)]\tLoss: 0.081203\n",
      "Train Epoch: 1 [44160/48000 (92%)]\tLoss: 0.156469\n",
      "Train Epoch: 1 [44800/48000 (93%)]\tLoss: 0.173034\n",
      "Train Epoch: 1 [45440/48000 (95%)]\tLoss: 0.052632\n",
      "Train Epoch: 1 [46080/48000 (96%)]\tLoss: 0.088564\n",
      "Train Epoch: 1 [46720/48000 (97%)]\tLoss: 0.071156\n",
      "Train Epoch: 1 [47360/48000 (99%)]\tLoss: 0.288047\n",
      "\n",
      "Test/validation set: Average loss: 0.0004, Accuracy: 11560/12000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    train(model, train_loader, loss_fn, i)\n",
    "    # Evaluate the model on the test set\n",
    "    test_model(model, val_loader, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After learning we evaluate the model on the _test set_ and save the resulting structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test/validation set: Average loss: 0.0019, Accuracy: 9664/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_model(model, test_loader, loss_fn)\n",
    "# Save the model\n",
    "save_model('conv.pt', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST_CNN(\n",
      "  (conv1): Conv2d(1, 15, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc): Linear(in_features=8640, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplmentary exercises\n",
    "\n",
    "1. Familiarize yourself with the code above and consult the PyTorch documentation when needed.\n",
    "2. Experiment with different NN architectures (also varying the convolutional parameters: size, stride, padding, etc) and observe the effect wrt. the loss/accuracy on the training and validation dataset (training, validation). Consult the reference PyTorch documentation as needed. **IMPORTANT:** ignore the test set at this stage (i.e., comment out the relevant lines above) so that the results for the test set do not influence your model choice.\n",
    "3. In the model above we use a simple gradient descent learning scheme. Try other types of optimizers (see https://pytorch.org/docs/stable/optim.html) and analyze the effect.\n",
    "4. If you feel adventurous, try investigating some of the other datasets that come prepacakged with PyTorch (see https://pytorch.org/vision/0.8/datasets.html). For instnce, for FashionMNIST you only need to change the dataloader from datasets.MNIST to datasets.FashionMNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
